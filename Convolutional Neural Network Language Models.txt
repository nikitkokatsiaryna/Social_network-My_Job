
3.2.1 Basic CNN network

The proposed CNN network is produced by injecting a convolutional layer right after the words in the input are projected to their embeddings (Figure 3.3). Rather than being concatenated into a long vector, the embeddings xi ∈ Rk are concatenated transversally producing a matrix x1:n ∈ Rn×k , where n is the size of the input and k is the embedding size. This matrix is fed to a time-delayed layer, which convolves a sliding window of w input vectors centered on each word vector using a parameter matrix W ∈ Rw×k . Convolution is performed by taking the dot-product between the kernel matrix W and each sub-matrix xi−w/2:i+w/2 resulting in a scalar value for each position i in input context. This value represents how much the words encompassed by the window match the feature represented by the filter W . A ReLU activation function is applied subsequently so negative activations are discarded. This operation is repeated multiple times using various kernel matrices W , learning different features independently. Here we tie the number of learned kernels to be the same as the embedding dimensionality k and set the zero-padding so that the output of this stage will be another matrix of dimensions n × k containing the activations for each kernel at each time step. For example, for window size w = 5, we pad the inputs with 2 zero units at each direction. The padding value is 3 for w = 7 and so forth. We always move the window by 1 unit per time lag (stride s = 1). The main reason for such setup is to keep the network structure identical to the baseline, and also to reduce the total number of parameters to be tuned during training.
Next, we add a batch normalization stage immediately after the convolutional output, which facilitates learning by addressing the internal covariate shift problem and regularizing the learned representations [31].
Finally, this feature matrix is directly fed into a fully connected layer that can project the extracted features into a lower-dimensional representation. This is different from previous work [14, 34], where a max-over-time pooling operation was used to find the most activated feature in the time series. Our choice is motivated by the fact that the max pooling operator loses the specific position where the feature was detected, which is important for word prediction.
After this initial convolutional layer, the network proceeds identically to the FFNN by feeding the produced features into a highway layer, and then, to a softmax output.
Training the network. Stochastic gradient descent (SGD) can be used to train our proposed language model. The objective of the network is to jointly learn the word embeddings, the convolutional kernels and maximising the likelihood of the training data. The loss function is similar to feed-forward and recurrent neural networks, which is the negativeloglikelihood function. The derivatives of the loss function with respect to the weights at each layer are computed with the back-propagation algorithm, which is described for feed-forward and convolutional neural networks in Chapter 2.

3.2.2 Extensions

This is the basic CNN architecture. We also conducted experiments with possible expansions to the basic model as follows.
First, motivated by the successes of image recognition networks that gain performance by stacking convolutional layers [26, 63], we aim at connecting learning features at local level by using stacked convolutional layers on top of each other (Multi-layer CNN or MLCNN). It is possible since after the first convolutional layer, the output has the same size and the input.
Second, we generalize the CNN by extending the shallow linear kernels with deeper multi-layer perceptrons, in what is called a MLP Convolution (MLPConv) structure [43]. Fundamentally, the convolutional kernels are universal function approximators that maps the embedding vectors into a value (feature). The shallow linear kernels can be upgraded into non-linear function approximators by using a fully connected neural network scanning through the input, which is illustrated in Figure 3.2. For comparison, the features extracted by a shallow linear convolution layer (the simple convolution described above) are exactly the hidden layer of the MLPConv layer. By adding another non-linear layer with the ReLU function, we can expect to have a better feature extractor (since the network is deeper).
Concretely, we implement MLPConv networks by using another convolutional layer with a 1 × 1 kernel on top of the convolutional layer output. This results in an architecture that is exactly equivalent to sliding a one-hidden-layer MLP over the input. Notably, we do not include the global pooling layer in the original network structure [43] which is specifically designed for computer vision applications.
Finally, we consider combining features learned through different kernel sizes (COM), as depicted in Figure 3.4. For example, we can have a combination of kernels that learn filters over 3-grams with others that learn over 5-grams. This is achieved simply by applying in parallel two or more sets of kernels to the input and concatenating their respective outputs [34]. It is notable that combining different kernels increase the size of hidden layers while keeping the same word vector size.

Figure 3.2: MLPConv architecture. The features are learned by a multi-layer perceptron after scanning through the network with convolution. The MLP weights are shared between kernels.

Figure 3.3: Convolutional layer on top of the context matrix.

Figure 3.4: Combining kernels with different sizes. We concatenate the outputs of 2 convolutional blocks with kernel size of 5 and 3 respectively.

Chapter 4 Experiments

4.1 Experiment details

4.1.1 Datasets for evaluation

We evaluate our model on three English corpora of different sizes and genres that have been used for language modeling evaluation before. The Penn Treebank contains one million words of newspaper text with 10K words in the vocabulary. We reuse the preprocessing and train/test/valid division from [48]. Europarl-NC is a a 64-million word corpus that was developed for a Machine Translation shared task [8], combining Europarl data (from parliamentary debates in the European Union) and News Commentary data. We preprocessed the corpus with tokenization and true-casing tools from the Moses toolkit [38]. The vocabulary is composed of words that occur at least 3 times in the training set and contains approximately 60K words.
Finally, we took a fragment of the ukWaC corpus which is constructed by crawling UK websites. The fragment contains 200 million words and we extracted a 200k-words vocabulary with the words that appear more than 5 times in the subset. The preprocessing step is similar to Europarl-NC. The validation and test set are different subsets of the ukWaC corpus, both containing 120k words.

4.1.2 Implementation Details

In order to observe the benefit of the convolutional layers, we implemented feed-forward neural language models as baselines, recurrent and long-short term memory models and finally, our convolutional networks. The implementation steps are as follows.
Vocabulary indexing. First, the words are converted from string to integers with a mapping table. A sentence {W1 , W2 . . . Wn } is then converted into an array of integers: {w1 , w2 . . . wn }, with wi being the index of word Wi in the vocabulary. Basically, the corpus (a very long sequence of words including new line tokens separated by space) is transformed into a very long tensor of integer, which is then used for extracting the samples to train our networks. At the end of the each sentence (each line), we put a token “eos” to inform the networks about the sentence boundary, which is a common practice in the literature. Also, it is important that all words that do not exist in the vocabulary are mapped into one token “unk”.
Sample extraction. In this step, we need to decide the input and output of the networks.  For the feed-forward architectures (baseline FFLM and CNN), each input example is a fixed length context and each output is one word only. The extraction process is performed by looping over the corpus, taking each word and its context. In each training iteration (feeding one example or one mini-batch described below to the network), we take one random sample from the training data. During testing, it is not necessary to randomise the order of the samples.
For recurrent architecture, we follow the method of Karpathy et al. [33] to extract the samples for the network. Concretely, we iteratively take a subset of the corpus (the long tensor) and ensure that the next subset is the successor of the current subset. For example, if the sequence length is 5 then the first sample is {w1 , w2 , w3 , w4 , w5 } and the next one is {w6 , w7 , w8 , w9 , w10 }. Note that, the RNNs receive one word at one step (w3 at time 3 for example) and produce the next word (w4 ).

Table 4.1: Hyper-parameters to be chosen when training neural network language models.

Batching. One important feature in neural network implementation is mini-batching. In the training algorithms described in Chapter 2, we showed the basic computation for propagating one sample forward and backward through the network. Additional performance can be achieved by propagating several examples (a mini-batch) at one through the network, because of two reasons [7]: First, the basic vector-matrix multiplication is transformed into matrix-matrix multiplication, which can be accelerated with Basic Linear Algebra Subprograms (BLAS) libraries or with GPU computing. Second, the distribution of a randomised mini-batch is closer to the distribution of the whole data, which benefits SGD optimisation. 
For the feed-forward architecture, a mini-batch is formed by randomly picking several examples together. In that case, the input of the network is a matrix of size B × m. B is the mini-batch size and m is the order of n-grams. The network output is a vector of size B. For recurrent networks, the input and output at each time step is a matrix of size B.
Training details. We train our models using Stochastic Gradient Descent (SGD) and we  reduce the learning rate by a fixed proportion every time the validation perplexity increases after one epoch. The values for learning rate, learning rate shrinking and mini-batch sizes as well as context size are fixed once and for all based on insights drawn from previous work [16, 24, 66] and through experimentation with the Penn Treebank validation set. Experimentally we found SGD to be efficient and adequately fast, while other learning algorithms involve additional hyper parameters (such as alpha in RMSprop [69]). The set of hyper parameters to be trained for models are showed in Table
Specifically, the learning rate is set to 0.05, with mini-batch size of 128 (we do not take the average of loss over the batch, and the training set is shuffled). We multiply the learning rate by 0.5 every time we shrink it and clip the gradients if their norm is larger than 12. The network parameters are initialized randomly on a range from −0.01 to 0.01 and the context size is set to 16. In Section 4.3 we show that this large context window is fully exploited.
For the base FFNN and CNN we study embedding sizes (and thus, number of kernels) k = 128, 256. For k = 128 we explore the simple CNN, incrementally adding NIN and COM variations (in that order) and, alternatively, using a ML-CNN. For k = 256, we only explore the former three alternatives. Because of the computational restriction, we chose the kernel size w, stride s and zero-padding z based on experiments in Penntreebank. For the kernel size, we set it to w = 3 words for the simple CNN (out of options 3, 5, 7, 9), whereas for the COM variant we use w = 3 and 5. However, we observed the models to be generally robust to this parameter. Dropout rates are tuned specifically for each combination of model and dataset based on the validation perplexity. We also add small dropout (0.05 − 0.15) when we train the networks on the small corpus (Penn Treebank).
The experimental results for recurrent neural network language models, such as Recurrent Neural Networks (RNN) and Long-Short Term Memory (LSTM), on the Penn Treebank are quoted from previous work; for Europarl-NC, we train our own models (we also report the performance of these in-house trained RNN and LSTM models on the Penn Treebank for reference). Specifically, we train LSTMs with embedding size k = 256 and number of layers L = 2 as well as k = 512 with L = 1, 2. We train one RNN with k = 512 and L = 2. To train these models, we use the published source code from [74]. Our own models are also implemented in Torch71 for easier comparison.
For all models trained on Europarl-NC and ukWaC, we speed up training by approximating the softmax with Noise Contrastive Estimation (NCE) [23], with the parameters being set following the previous work from [12]. Concretely, for each predicted word, we sample 10 words from the unigram distribution, and the normalization factor is such that ln Z = 9 2 .
For comparison, we also implemented the origial version of the FFNN [4] with two hidden layers with the size of 2 times the embedding size (k). These networks do not have dropout as well as the highway layers in our baseline, yet still share the same number of parameters.

4.2 Results

Our experimental results are summarized in Table 4.2.
First of all, we can see that even though the FFNN gives a very competitive performance, the addition of convolutional layers is clearly effective to increase it even further3 . Concretely, we observe a solid 13% reduction of perplexity compared to the feed-forward network after using Network-in-Network in all setups for both corpora. CNN alone yields a 6% improvement, while MLPConv, in line with our expectations, adds another 5% reduction in perplexity. A final (smaller) improvement comes from combining kernels of size 3 and 5, which can be attributed to a more expressive model that can learn patterns of n-grams of different sizes.In contrast to the successful two variants above, the multi-layer CNN did not help in better capturing the regularities of text, but rather the opposite: the more convolutional layers were stacked, the worse the performance. This also stands in contrast to the tradition of convolutional networks in Computer Vision, where using very deep convolutional neural networks is key to having better models. In text, deep convolution for text representation is rather rare, and has only been applied in sentence representation [32]. We conjecture that the reason why deep CNNs may not be so effective for text could be the non-recursive nature of the textual data after convolution. In other words, the convolution output for an image can be construed to be a new image, which yet again can be subject to new convolution operations, whereas the textual counterpart may no longer have the same property.
Regarding the comparison with a stronger LSTM, our models can perform competitively under the same embedding dimension (e.g. see k = 256 of k = 512) on the first two datasets. However, the LSTM can be easily scaled using larger models, as shown in Zaremba et al. [74], which gives the best known results to date. This is not an option for our model which heavily overfits with large hidden layers (around 1000) even with very large dropout values. Furthermore, the experiments on the larger ukWaC corpus show an advantage for the LSTM, which seems to be more efficient at harnessing this volume of data. On the other hand, the improvement of the convolutional model with respect to the FFNN becomes even more dramatic (more than 20%) using similarly sized models.

Table 4.2: Results on Penn Treebank and Europarl-NC. Figure of merit is perplexity (lower is better). Legend: k: embedding size (also number of kernels for the convolutional models and hidden layer size for the recurrent models); w: kernel size; val: results on validation data; test: results on test data; #p: number of parameters; L: number of layers.

Figure 4.1: Some example phrases that have highest activations for 8 example kernels (each box), extracted from the validation set of the Penn Treebank. Model trained with 256 kernels for 256-dimension word vectors.

To sum up, we have established that the results of our CNN model are well above those of simple feed forward networks and recurrent neural networks. While they are below state of the art LSTMs, they are able to perform competitively with them for small and moderatesize models. Scaling to larger sizes may be today the main roadblock for CNNs to reach the same performances as large LSTMs in language modeling.

4.3 Model Analysis

In what follows, we obtain insights into the inner workings of the CNN by looking into the linguistic patterns that the kernels learn to extract and also studying the temporal information extracted by the network in relation to its prediction capacity.
Learned patterns. To get some insight into the kind of patterns that each kernel is learning to detect, we fed trigrams from the validation set of the Penn Treebank to each of the kernels, and extracted the ones that most highly activated the kernel. Some examples are     shown in Figure 4.1. Since the word windows are made of embeddings, we can expect patterns with similar embeddings to have close activation outputs. This is borne out in the analysis: The kernels specialize in distinct features of the data, including more syntacticsemantic constructions (cf. the “comparative kernel” including as . . . as patterns, but also of more than) and more lexical or topical features (cf. the “ending-in-month-name” kernel). Even in the more lexicalized features, however, we see linguistic regularities at different levels being condensed in a single kernel: For instance, the “spokesman” kernel detects phrases consisting of an indefinite determiner, a company name (or the word company itself) and the word “spokesman”. We hypothesize that the convolutional layer adds an “I identify one specific feature, but at a high level of abstraction” dimension to a feed-forward neural network, similarly to what has been observed in image classification [39].

Figure 4.2: The distribution of positive weights over context positions, where 1 is the position closest to the predicted word.

Temporal information. To the best of our knowledge, the longest context used in feedforward language models is 10 [25], where no significant change in terms of perplexity was observed for bigger context sizes, even though in that work only same-sentence contexts were considered. In our experiments, we use a larger context size of 16 while removing the sentence boundary limit (as commonly done in n-gram language models) such that the network can take into account the words in the previous sentences.
To analyze whether all this information was effectively used, we took our best model, the CNN-NIN-COM model with embedding size of 256 (fourth line, second block in Table 4.2), and we identified the weights in the model that map the convolutional output (of size n × k) to a lower dimensional vector (the “mapping” layer in Figure 3.3). Recall that the output of the convolutional layer is a matrix indexed by time step and kernel index representing the activation of the kernel when convolved with a window of text centered around the given time step. Thus, output units of the above mentioned mapping predicate over an ensemble of kernel activations for each time-step . We can identify the patterns that they learn to detect by extracting the time-kernel combinations for which they have positive weights (since we have ReLU activations, negative weights are equivalent to ignoring a feature). First, we asked ourselves whether these units tend to be more focused on the time-steps closer to the target or not. To test this, we calculated the sum of the positive weights for each position in time using an average of the mappings that correspond to each output unit. The results are shown in Figure 4.2. As could be expected, positions that are close to the token to be predicted have many active units (local context is very informative; see positions 2-4). However, surprisingly, positions that are actually far from the target are also quite active (see positions 10-14, with a spike at 11). It seems like the CNN is putting quite a lot of effort on characterizing long-range dependencies.
Next, we checked that the information extracted from the positions that are far in the past are actually used for prediction. To measure this, we artificially lesioned the network so it would only read the features from a given range of timesteps (words in the context). To lesion the network we manually masked the weights of the mapping that focus on times outside of the target range by setting them to zero. We started using only the word closest to the final position and sequentially unmasked earlier positions until the full context was used again. The result of this experiment is presented in Figure 4.3, and it confirms our previous observation that positions that are the farthest away contribute to the predictions of the model. The perplexity drops dramatically as the first positions are unmasked, and then decreases more slowly, approximately in the form of a power law (f (x) ∝ x−0.9 ). Even though the effect is smaller, the last few positions still contribute to the final perplexity.

Figure 4.3: Perplexity change over position, by incrementally revealing the Mapping’s weights corresponding to each position.

Chapter 5

Related Work

Provided our proposed models for language modeling using convolutional neural networks, we cover the related works that apply convolutional neural networks in NLP problems.
Time-delay neural networks or convolutional neural networks (CNNs) were originally designed to deal with hierarchical representation in signal processing [71] and computer vision [42]. Deep networks with many stacked convolutional layers have been successfully applied in image classification and understanding [26, 63]. In such systems the convolutional kernels manage to learn to detect visual features at both local and more abstract levels.
In NLP, CNNs have been mainly applied to static classification task for discovering latent structures in text. Kim [34] use a CNN to tackle sentence classification, with competitive results. This work also introduces kernels with varying window sizes to learn complementary features at different aggregation levels. Kalchbrenner et al. [32] propose a convolutional architecture for sentence representation that vertically stacks multiple convolution layers, each of which can learn independent convolution kernels. CNNs with similar structures have also been applied to other classification tasks, such as semantic matching [30], relation extraction [52] and information retrieval [62].
In contrast, Collobert et al. [14] explore a CNN architecture to solve various sequential and non-sequential NLP tasks such as part-of-speech tagging, named entity recognition and also language modeling. This is perhaps the work that is closest to ours in the existing literature. However, their model differs from ours in that it uses a max-pooling layer that picks the most activated feature across time, thus ignoring temporal dependencies, whereas we explicitly avoid doing so. More importantly, the language models trained in this work are only evaluated through downstream tasks and through the quality of the learned word embeddings, but not on the sequence prediction task itself.
Besides being applied on word-based sequences, the convolutional layers have also been used to model sequences at the character level. Kim et al. [35] propose a recurrent language model that replaces the word-indexed projection matrix with a convolution layer fed with the character sequence that constitutes each word to find morphological patterns. The main difference between their and our work is that we consider words as the smallest linguistic unit, and thus apply the convolutional layer at the word level. Also, convolutional layers can be applied to a full sentence whose fundamental units are characters [76]. Stateof-the-art results in text classification has been observed by using very deep convolutional neural networks [15].
In this work we focus on statistical language modeling, which differs from most of the tasks where CNNs have been applied before in multiple ways. First, the input normally consists of incomplete sequences of words rather than complete sentences. Second, as a classification problem, it features an extremely large number of classes (the words in a large vocabulary). Finally, temporal information, which can be safely discarded in many settings with little impact in performance, is critical here: An n-gram appearing close to the predicted word may be more informative, or yield different information, than the same n-gram appearing several tokens earlier.

Chapter 6

Conclusion

In this work, we have investigated the use of Convolutional Neural Networks for language modeling, a sequential prediction task. We incorporate a CNN layer on top of a strong feedforward model enhanced with modern techniques like Highway Layers and Dropout. Our results show a solid 11-26% improvement in perplexity with respect to the feed-forward model across two corpora of different sizes and genres when the model uses Network-inNetwork and combine kernels of different window sizes. However, even without these additions we show CNNs to effectively learn language patterns to significantly decrease the model perplexity.
In our view, this improvement responds to two key properties of CNNs, highlighted in the analysis. First, as we have shown, they are able to integrate information from larger context windows, using information from words that are as far as 16 positions away from the predicted word. Second, as we have qualitatively shown, the kernels learn to detect specific patterns at a high level of abstraction. This is analogous to the role of convolutions in Computer Vision. The analogy, however, has limits; for instance, a deeper model stacking convolution layers harms performance in language modeling, while it greatly helps in Computer Vision. We conjecture that this is due to the differences in the nature of visual vs. linguistic data. The convolution creates sort of abstract images that still retain significant properties of images. When applied to language, it detects important textual features but distorts the input, such that it is not text anymore.
As for recurrent models, even if our model outperforms RNNs, it is well below state-ofthe-art LSTMs. Since CNNs are quite different in nature, we believe that a fruitful line of future research could focus on integrating the convolutional layer into a recurrent structure for language modeling, as well as other sequential problems, perhaps capturing the best of both worlds.
